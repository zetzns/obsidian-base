
> [!question] 
> Всегда ли решение задачи регрессии единственно? 

> [!problem]
> Вообще говоря, ***нет***. Так, если ***в выборке два признака будут линейно зависимы*** (и следовательно, ранг матрицы будет меньше D), то `гарантировано найдётся такой вектор весов` ν что $⟨v, x_i⟩=0,\ \forall x_i$, а значит $]\ \exists w - solve => w+\alpha v - solve$.

То есть решение не только не обязано быть уникальным, так ещё может быть сколь угодно большим по модулю. Это создаёт вычислительные трудности. Малые погрешности признаков сильно возрастают при предсказании ответа, а в градиентном спуске накапливается погрешность из-за операций со слишком большими числами.

> [!important] 
> В реально мире возможно `мультиколлинеарность`, которая породит проблему $Xv\approx 0$ для $v$, а значит $X^TXv\approx0 =>$ матрица $X^TX$ близка к вырожденной и это приведёт к росту компонент и погрешностей вычислений. 
