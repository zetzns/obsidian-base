
> [!info] 
> В логистической регрессии для двух классов мы строили линейную модель $$b(x)=\langle w,x \rangle + w_{0}$$ 
> А затем переводили её в прогноз в вероятность с помощью сигмойды $\sigma(z)=\frac{1}{1+e^{-z}}$.

> [!idea] 
> Пусть теперь мы построили $K$ линейных моделей $$b_k(x) = \langle w_k,x \rangle + w_{0k},$$
> каждая из которых даёт оценку принадлежности к определённому классу. Теперь, для нормирования, воспользуемся `softmax`-ом: $$softmax(z_1,...,z_K) = \left( \frac{e^{z_1}}{\sum_{k=1}^Ke^{z_k}},...,\frac{e^{z_K}}{\sum_{k=1}^Ke^{z_k}} \right)$$
> Вероятность же, $$P(y=k|x,w)=\frac{exp(\langle w_k,x \rangle + w_{0k})}{\sum_{j=1}^Kexp(\langle w_j,x \rangle + w_{0j})}$$

> [!important] 
> Обучать эти веса предлагается с помощью метода максимального правдоподобия: так же, как и в случае с двухклассовой логистической регрессией: $$\sum_{i=1}^N\log P(y=y_i|x_i,w)\longrightarrow \max_{w_1,...,w_K}$$
