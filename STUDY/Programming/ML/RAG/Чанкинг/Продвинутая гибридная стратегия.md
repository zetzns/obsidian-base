```python
def hybrid_chunking(text, max_tokens=400, overlap=50):
    """
    Combines semantic awareness with token control
    """
    # First, try semantic chunking
    semantic_chunks = semantic_chunk(text, max_tokens)
    
    final_chunks = []
    
    for chunk in semantic_chunks:
        chunk_tokens = len(tokenizer.encode(chunk))
        
        # If semantic chunk is too large, fall back to token chunking
        if chunk_tokens > max_tokens:
            print(f"Chunk too large ({chunk_tokens} tokens), using token chunking")
            token_chunks = chunk_by_tokens(chunk, chunk_size=max_tokens, overlap=overlap)
            final_chunks.extend(token_chunks)
        else:
            final_chunks.append(chunk)
    
    return final_chunks
```

> [!important]
> - Сначала выполняется семантический чанкинг
> - Если фрагмент слишком большой — применяется фиксированный токенный чанкинг
> - Комбинирует семантическое понимание и точный контроль длины