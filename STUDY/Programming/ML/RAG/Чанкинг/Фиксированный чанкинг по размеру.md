```python
def chunk_by_tokens(text, chunk_size=300, overlap=50):
    tokens = tokenizer.encode(text)
    chunks = []
    for i in range(0, len(tokens), chunk_size - overlap):
        chunk_tokens = tokens[i:i + chunk_size]
        chunks.append(tokenizer.decode(chunk_tokens))
    return chunks
```

> [!important]
> - Сначала преобразует весь текст в токены
> - Разрезает строго по токенным границам, независимо от содержания
> - Создаёт перекрывающиеся окна фиксированного размера