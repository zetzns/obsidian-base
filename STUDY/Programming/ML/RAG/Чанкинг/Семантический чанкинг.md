```python
def semantic_chunk(text, max_tokens=400):
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = ""
    current_tokens = 0
    
    for sentence in sentences:
        sentence_tokens = len(tokenizer.encode(sentence))
        if current_tokens + sentence_tokens <= max_tokens:
            current_chunk += sentence + " "
            current_tokens += sentence_tokens
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + " "
            current_tokens = sentence_tokens
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks
```

> [!important]
> - Сначала разбивает текст на предложения
> - Группирует предложения, пока не достигнет лимита токенов
> - Никогда не разрывает текст внутри предложения