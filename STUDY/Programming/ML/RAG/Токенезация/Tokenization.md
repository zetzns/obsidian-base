
> [!def] 
> Прежде чем преобразовать текст в `embeddings`, он должен быть `токенезирован` - разбит на более мелкие единицы, которые модель способна понять.

> [!list]
> **Ключевые концепции токенизации:**
> 
> - **Ограничения по числу токенов:** большинство моделей имеют ограничения на длину входа (например, 512 токенов для моделей на основе BERT).
> - **Согласованность токенизатора:** используйте один и тот же токенизатор как для индексирования, так и для выполнения запросов.
> - **Стратегия чанкинга:** делите длинные документы, при этом стараясь сохранять семантические границы.

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokens = tokenizer.encode("Your text here", max_length=512, truncation=True)
```
