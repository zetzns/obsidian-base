
> [!key] 
> Фундаментальное заблуждение при построении RAG — думать, что сами эмбеддинги могут служить контекстом для LLM. Это ключевая архитектурная ошибка.

```python
# Это НЕ будет работать
query_embedding = embed_text("What is photosynthesis?")
similar_embeddings = vector_db.search(query_embedding, k=5)
# ❌ Невозможно передать эмбеддинги напрямую в LLM
response = llm.generate(embeddings=similar_embeddings)  # Ошибка!
```

```python
# А вот это будет работать
query_embedding = embed_text("What is photosynthesis?")
similar_chunks = vector_db.search(query_embedding, k=5)
# ✅ Нужно извлечь исходный текст
context_text = [chunk.metadata['text'] for chunk in similar_chunks]
response = llm.generate(query="What is photosynthesis?", context=context_text)
```

> [!important]
> Почему это важно:
> 
> - Эмбеддинги — **сжатые, потерянные** представления, передающие только смысл.
> - LLM обучены на **тексте**, а не на векторах.
> - **Нет функции “обратного эмбеддинга”** — невозможно восстановить текст из вектора.
> - Пространство эмбеддингов оптимизировано для **семантического сходства**, а не для восстановления информации.

