```python
import hashlib
from functools import lru_cache

class EmbeddingCache:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.cache_ttl = 86400  # 24 часа
    
    def get_content_hash(self, text):
        return hashlib.sha256(text.encode('utf-8')).hexdigest()
    
    def get_embedding(self, text):
        content_hash = self.get_content_hash(text)
        cache_key = f"embed:{content_hash}"
        
        cached_embedding = self.redis.get(cache_key)
        if cached_embedding:
            return json.loads(cached_embedding)
        
        embedding = self.embedding_model.encode(text)
        
        self.redis.setex(
            cache_key, 
            self.cache_ttl, 
            json.dumps(embedding.tolist())
        )
        
        return embedding
```

Использование:

```python
embedding_cache = EmbeddingCache(redis_client)

def process_document(document_chunks):
    embeddings = []
    for chunk in document_chunks:
        embedding = embedding_cache.get_embedding(chunk.text)
        embeddings.append(embedding)
    return embeddings
```

Без кэша – расточительная стратегия:

```python
documents = [
    "The mitochondria is the powerhouse of the cell",
    "Photosynthesis converts sunlight into energy", 
    "The mitochondria is the powerhouse of the cell",
    "DNA contains genetic information",
    "The mitochondria is the powerhouse of the cell"
]

for doc in documents:
    embedding = model.encode(doc)  # Повторное создание, дорогостоящее
```

Решение:

```python
cache = EmbeddingCache(redis_client)

for doc in documents:
    embedding = cache.get_embedding(doc)  # Только уникальные эмбеддинги
```