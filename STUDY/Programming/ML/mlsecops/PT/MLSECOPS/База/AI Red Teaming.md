![[Pasted image 20251120161116.png]]

> [!security] 
> `AI Red Teaming` - это структурированный, проактивный и состязательный подход к тестированию ИИ, который имитирует тактики, техники и процедуры реальных противников, исследуя уязвимости во всех компонентах AI-системы: данных, моделях, пайплайнах, инфраструктуре и поведении.

> [!def] 
> `Структурированная adversial-оценка` безопасности AI: моделируем реальные TTP злоумышленников.

> [!important] 
> `Фокус на данных, модели и системном поведении`, а не на коде - принципиальное отличие от классического пентеста.

> [!goal] 
> Цель: **выявить атакуемые точки через имитацию действий реальных атакующих**

> [!security] 
> Подход системного мышления: оцениваем всю цепочку: данные -> тренировка -> деплой -> API -> пользователи






